- https://www.youtube.com/watch?v=589nCGeWG1w
- https://ai-ml-analytics.com/encoding 
- https://medium.com/anolytics/all-you-need-to-know-about-encoding-techniques-b3a0af68338b

- its known that we use encoding to convert categorical data into numerical form that the model can understand
- before we start encoding we need to know types of categorical data
##### Categorical data
###### 1. Nominal Data
- Nominal categorical variables are those for which <u>we do not have to worry about the arrangement</u> of the categories.
- suppose we have a gender column with categories as Male and Female.
###### 2. Ordinal Data
- Ordinal categories are those in which <u>we have to worry about the rank.</u> 
- These categories can be rearranged based on ranks.
* Suppose in a dataset there is an education column which we will use to predict the salary of the person. 
* The education column has categories like ‘bachelors’,’masters’,’PHD’. Based on the above categories we can rearrange this and assign ranks to each category. 
* Based on the education level ‘PHD’ will get the highest rank (PHD-1, masters-2, bachelors-3)

# Encoders
- so encoders will be chosen according to type of the categorical data 
- it means there is 2 types 
	- Ordinal encoding
	- Nominal encoding

![[Pasted image 20240910114705.png | center]]

---
# Nominal Encoding 
- One-hot encoding is the most widely used categorical encoding technique. It is suitable for nominal categorical variables
- The idea behind one-hot encoding is to represent each category as a binary vector.
###### How it works
- For each category in a categorical column, a new binary column is created
- The binary column will have a value of 1 if the class is present, else it will be zero
---
<h2 align = "center">  One Hot encoding </h2>
- One-hot encoding is the most widely used categorical encoding technique. It is suitable for nominal categorical variables, where the categories have no inherent order or relationship. 
- The idea behind one-hot encoding is to represent each category as a binary vector.

![[Pasted image 20240917120223.png | center]]
<br>

**Implementation**
- one hot encoder may be implemented by 2 ways , **First** one ---> pandas 

```python
import pandas as pd

# Sample dataset with a categorical column
data = {'Color': ['Red', 'Blue', 'Green', 'Red', 'Green']}
df = pd.DataFrame(data)

# prefoem one hot encoding
one_hot_encoded = pd.get_dummies(df, columns=['Color'] , drop_first = "True").astype(int)
```
- `drop_first` : this parameter is used to apply the dummy trap in pandas 

<br> 

- the **Second** one is by Sklearn library 
```python
# Sample data
data = pd.DataFrame({
    'color': ['red', 'green', 'blue', 'green'],
    'size': ['S', 'M', 'L', 'M']
})

# Initialize the OneHotEncoder with the option to get feature names
encoder = OneHotEncoder(drop = "first" ,sparse_output=False)  # Ensure output is dense, not sparse

# Fit and transform the data
encoded_data = encoder.fit_transform(data)

# Get the feature names generated by the OneHotEncoder
column_names = encoder.get_feature_names_out(input_features=data.columns)

# Create a DataFrame with the encoded data and the correct column names
encoded_df = pd.DataFrame(encoded_data, columns=column_names)

# convert into int
encoded_df = encoded_df.astype(int)

# Show the resulting DataFrame
(encoded_df)

```
- `sparse_output` : this parameter is set to `False` to ensure dense data (not skipping the zeros)
- `drop` : to apply dummy trap

>[!Hint] # Hint
> - we need to change the data type after encoding , because `sklearn` encoding convert the data to an array

### Pros
- It preserves all information about the categories and doesn’t introduce any ordinal relationship
### Cons
- It can lead to a high dimensionallity problem when dealing with a large number of classes
### When to use
- Ideally for categorical features with less than 10 categories

---
<h2 align = "center">  Target Encoding </h2>

- Target encoding, also known as mean encoding, involves replacing each category with the mean (or some other statistic) of the target variable for that category.
- **How it works** 
	- Calculate the mean of the target variable for each category.
	- Replace the category with its corresponding mean value.

**Implementation**
- before we use it we need to check if we have the library installed 
- if not we shall install it
```bash
pip install category_encoder 
```

- target encoder
```python
import pandas as pd
from sklearn.model_selection import train_test_split
import category_encoders as ce

# Generate a dummy dataset with categorical variables
data = {
    'Color': ['Red', 'Blue', 'Green', 'Red', 'Red', 'Blue', 'Green'],
    'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small', 'Small', 'Medium'],
    'Label': [1, 0, 1, 1, 0, 0, 1]
}

df = pd.DataFrame(data)

# Split the data into training and test sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Initialize the MeanEncoder
mean_encoder = ce.TargetEncoder()

# Fit the encoder on the training data
mean_encoder.fit(train_df[['Color', 'Size']], train_df['Label'])

# Transform both the training and test datasets
train_encoded = mean_encoder.transform(train_df[['Color', 'Size']])
test_encoded = mean_encoder.transform(test_df[['Color', 'Size']])

# Display the encoded datasets
print("Training Data (After Mean Encoding):\n", train_encoded.to_markdown())
print("\nTest Data (After Mean Encoding):\n", test_encoded.to_markdown())
```

- output data
<p align = "center">Training Data (After Mean Encoding):</p>

| index |  Color   |   Size   |
|:-----:|:--------:|:--------:|
|   5   | 0.521935 | 0.514889 |
|   2   | 0.65674  | 0.652043 |
|   4   | 0.585815 | 0.514889 |
|   3   | 0.585815 | 0.65674  |
|   6   | 0.65674  | 0.65674  |
<p align = "center">Test Data (After Mean Encoding):</p>

| index |    Color |   Size   |
|:-----:| --------:|:--------:|
|   0   | 0.585815 | 0.514889 |
|   1   | 0.521935 | 0.65674  |
