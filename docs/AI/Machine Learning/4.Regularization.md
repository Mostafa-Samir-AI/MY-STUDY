---
author: Mostafa Samir Ali
tags:
  - AI
  - Machine_learning
---

# Regularization Techniques
- we have many regularization techniques that we use in machine learning <u>mostly when we have small dataset</u> or <u>when the linear correlation is weak</u> 
## Types of regularization
1. L1 LASSO regression
2. L2 Ridge regression
3. Elastic net
![[Screenshot 2024-10-14 140917.png | center]]
## What exactly is L1 and L2 regularization?

- **L1 regularization**, also known as **LASSO regression** adds the absolute value of each coefficient as a penalty term to the loss function.
- **L2 regularization**, also known as **Ridge regression** adds the squared value of each coefficient as a penalty term to the loss function.

## Why do we need L1 and L2 regularization?

In order to understand why we need any sort of regularization, let’s go through a quick example.

Let’s say we wanted to predict people’s height with a dataset that included several predictors such as: **weight**, **salary**, **ethnicity** and **eyesight**. Our linear regression equation would be like the one below.

![[Screenshot 2024-10-14 141135.png | center]]

*While WE KNOW* ==some of these predictors are incorrect and would not provide any useful insight== into someone’s height, linear regression would force a way in doing so by minimizing the loss function, in this case, let’s say RSS (Residual Sum of Squares). This leads to overfitting, which essentially means that even the noise within the dataset is being modelled.

**Regularization combats this by adding a penalty term that can help disregard or weaken irrelevant predictors like eyesight and salary from the model.**

# L1 LASSO regression
- also known as <u>Least Absolute Shrinking and Separation Operation</u> 

![[Screenshot 2024-10-14 143619.png | center]]
## How different values of **λ affect the model**

**_λ_** represents the strength of the regularization and is a hyperparameter chosen by the user. Equally, if set to 0, simply transforms into an Ordinary Least Squares (OLS) problem.

Increasing **_λ_** would require the coefficient **β** to decrease proportionally as we are still trying to minimise the function as a result at **_λ_ =** ∞, the coefficients **β** would be shrunk to 0 as **_λ_** would dominate the equation.

![[Screenshot 2024-10-14 143746.png | center]]
The choice of **_λ_** is important because when chosen correctly can be very insightful. This is usually done with a method called _k_-fold cross-validation

## what is used for

1. **Feature selection:** By penalising the absolute values of the coefficients, L1 regularization attempts to drive the coefficient values of less relevant features towards 0, thus, **keeping only the relevant features**.
2. **Sparsity:** As more coefficients are driven to values of 0, less features are contained in the model. A sparse model would allow a more computationally efficient model with easier interpretability.
3. **Collinearity:** If there are features that are also correlated to other features, L1 regularization would choose only one of these features to include in the model and reduce the other’s coefficient to 0.
4. **Model simplification:** Given all the above advantages, the resulting model from L1 regularization would be simpler, generalised and easier to interpret.

## practical 
- importing the lasso regression
```python
# import lasso regression
from sklearn.linear_model import Lasso 
```
- instance of the model
```python
# model
model = Lasso()
```
- training model
```python
# training
model.fit(x_train , y_train)
```
- making predictions
```python
# y_pred 
y_pred = model.predict(x_test)
```

##### lets see if the results
- Lasso regression supposed to minimize features and see the most correlated features to the model
- how are we gonna check that
	- we gonna list all the coefs and plot them with the names related to (each feature and its coef)
```python
# making list of coefs
coef = model_lasso.coef_

# plotting 
plt.plot(range(len(feature_names)) , coef)
plt.xticks(range(len(feature_names)) , feature_names , rotation = 60)
```

![[Screenshot 2024-10-14 150427.png | center]]
- as we see , the `LSTAT` and `DIS` features are the most correlated to our model output
- lets check by plotting
```python
plt.scatter(data["LSTAT"] , data["MEDV"])
```
![[Screenshot 2024-10-14 150618.png | center]]
```python
plt.scatter(data["DIS"] , data["MEDV"])
```
![[Screenshot 2024-10-14 150643.png | center]]

- as we can see on the plot these 2 features are related to the output

# L2 Ridge regularization
* L2 regularization, also known as Ridge regression is used for a variety or reasons such as:

- Feature maintenance
- Stability
- Collinearity

![[Screenshot 2024-10-14 151204.png | center]]

## How different values of λ affect the model

- **_λ_** represents the strength of the regularization and is a hyperparameter chosen by the user. Equally, if set to 0, simply transforms into an Ordinary Least Squares (OLS) problem.

- Increasing **_λ_** would require the coefficient **β** to decrease proportionally as we are still trying to minimize the function as a result at **_λ_ =** ∞, the coefficients **β** would be shrunk to near 0 values as **_λ_** would dominate the equation. **However, please note that unlike in L1 regularization, the coefficients rarely shrink to exactly 0.**
![[Screenshot 2024-10-14 143746.png | center]]
## Why is the penalty term squared?

- By using a squared penalty instead of a linear penalty, the model is encouraged to shrink the coefficients downer to smaller values. 
- This allows for a smoother and less complex model. 
- Additionally, since large errors are squared, they contribute more to the loss function helping stabilize the model and reducing the impact of outliers and noise within the data.

## practical

- importing the lasso regression
```python
# import ridge regression
from sklearn.linear_model import Ridge 
```
- instance of the model
```python
# model
model = Ridge()
```
- training model
```python
# training
model.fit(x_train , y_train)
```
- making predictions
```python
# y_pred 
y_pred = model.predict(x_test)
```

>[!HINT]  pro tip
> - Ridge regression works better when we have dataset that have alot of important features
> - while Lasso works better when we want to neglect non important features
> - so Ridge is used for see better correlation while Lasso is used for feature engineering