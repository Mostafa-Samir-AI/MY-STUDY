---
author: Mostafa Samir Ali
tags:
  - AI
  - Machine_learning
---

# Decision tree 

![Decision Tree Diagram](https://process.videosummarizerai.com/static/diagrams_svg_png/diagram_e70ca3da086b4ed6ab0715f84ed702e7.png)
 [DOWNLOAD DIAGRAM](https://process.videosummarizerai.com/static/diagrams_svg_png/diagram_e70ca3da086b4ed6ab0715f84ed702e7.png)

---

- ### Decision Tree Algorithm

* Decision Tree algorithm belongs to the family of supervised learning algorithms. Unlike other supervised learning algorithms, the decision tree algorithm can be used for solving **regression and classification problems** too.
* The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by **learning simple decision rules** inferred from prior data(training data).
- In Decision Trees, for predicting a class label for a record we start from the **root** of the tree. 
- We compare the values of the root attribute with the record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.


# Important terminology
1. **Root node** : It represents the entire population or sample and this further gets divided into two or more homogeneous sets.
2. **Parent and Child Node:** A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node.
3. **Decision Node:** When a sub-node splits into further sub-nodes, then it is called the decision node.
4. **Leaf / Terminal Node:** Nodes do not split (end of the Decision tree), is called Leaf or Terminal node.
5. **Branch / Sub-Tree:** A subsection of the entire tree is called branch or sub-tree.
6. **splitting:** It is a process of dividing a node into two or more sub-nodes.
7. **Pruning:** When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting.

![[sh3405xi.bmp | center]]

# Types of Decision Trees
1. Classification Decision Tree
2. Regression Classification Tree
## Classification Decision Tree
- Classification Trees work on categorical Discrete target values
<h3 align = "center">How to build one</h3>

![[Screenshot 2025-01-01 160348.png | center]]

- we have a data of 3 features 
	- loves popcorn
	- loves soda
	- age
- and the target --> we want to predict if the people in data loves soda cold as ice or not

> [!NOTE]
> - the target is discrete categorical values , *yes* or *no*
- we will use this data to build our DT
---
#### Step 1 : splitting among all features
- **Now the first thing we need to do is to know which one of the 3 features should we select to ask in the root node**
- we will do a simple tree structure for each feature
- we will split the feature into n-branches (n-categories) and then evaluate the target according to this split

>[!info]
> - divide popcorn into 2 categories 
> 	- $1^{st}$ is love popcorn
> 	- $2^{nd}$ do not love popcorn
> 	- these 2 are the leaf nodes
> - then for each leaf node we will calculate the target Decision for each leaf node
> 	- if they *love soda cool as ice* , add 1 to *yes*
> 	- if not add 1 to *no*



![[Screenshot 2025-01-01 161526.png | center]]

- this process will be done for all categorical features
>[!summary] ### important info
> - there are 3 of 4 nodes that have a mixture of classes *yes* and *no*
> - these nodes stated as impure
> - only one node that have 1 class , called pure node

the next thing to do is to **Quantify** the impurity for each feature and leaf below

#### Step 2 : calculating impurity
- we use many methods to calculate the impurity of leaves and features
- but at this example we will use GINI impurity
---
<h3 align = "center">GINI impurity for categorical data</h3>

- to calculate GINI impurity we need to
<br><br>

1. Calculate the GINI impurity for each sub-node (leaf):
   - GINI impurity for a single leaf:
     $$
     GINI = 1 - P(\text{class}_1)^2 - P(\text{class}_2)^2
     $$
   - $P(class)$ : probability of class

2. Calculate the total GINI impurity:
     $$
     \text{Total GINI} = \left(\frac{W(\text{leaf}_1)}{\text{total}} \times \text{leaf}_1\ \text{GINI}\right) + \left(\frac{W(\text{leaf}_2)}{\text{total}} \times \text{leaf}_2\ \text{GINI}\right)
     $$
   - $w(leaf)$ : weight of the leaf 
   - $leaf\ GINI$ : GINI impurity calculated to each leaf


### GINI for a leaf

![[Screenshot 2025-01-01 213839.png | center]]


### total GINI

![[Screenshot 2025-01-01 214450.png | center]]

---
<h3 align = "center">GINI impurity for continuous data</h3>

#### Steps
1. arrange the numerical feature in ascending order

| age | love cool as ice |
|:---:|:----------------:|
|  7  |        no        |
| 12  |        no        |
| 18  |       yes        |
| 35  |       yes        |
| 38  |       yes        |
| 50  |        no        |
| 83  |        no        |

2. calculate the average foreach 2 corresponding values

| average |
| :-----: |
|   9.5   |
|   15    |
|  26.5   |
|  36.5   |
|   44    |
|  66.5   |

3. take an average value and set as threshold 

![[Screenshot 2025-01-04 083757.png | center]]

4. calculate the *GINI* foreach leaf then *total GINI*

![[Screenshot 2025-01-04 083815.png | center]]

5. repeat for each average 

![[Screenshot 2025-01-04 083320.png | center]]

6. <u>pick up the average value with the least GINI</u> 

---
#### 3. Step 3 : picking the root feature
- now after we calculated the GINI for each feature we will pick the least GINI value 

![[Screenshot 2025-01-04 084750.png | center]]

- because **loves soda**  have the lowest impurity , we will pick it as the root feature
- we can now split the tree based on feature **loves soda** and target **loves cool as ice**

#### Step 4 : next split
- we now splitted the data into 2 branches (according to CART algorithm)
- now we need to determine which feature next we will use to further splitting 
- there are 2 features 
	- popcorn
	- age

- <u>the procedure is to try to split the data according to both features , calculate the GINI and decide which to use based on lowest GINI value </u>

![[Pasted image 20250104090718.png | center]]


> [!info] NOTE
> - at a certain point we don't need to split data anymore , making a leaf node
> - the output of the leaf is decided by the majority class in the leaf 
>
>> [!summary] do not love cool as ice 
>> ![[Screenshot 2025-01-04 091104.png | center]]

---
# code
## algorithm parameters

In decision trees (DT), especially when using the `DecisionTreeClassifier` or `DecisionTreeRegressor` from `sklearn`, several hyperparameters can be tuned to control the tree's growth and complexity. Here's an explanation of the parameters you mentioned:

### 1. **max_depth**

- **Definition**: Limits the maximum depth of the tree. A depth of `None` (default) means nodes are expanded until all leaves are pure (contain only one class or value) or until all leaves contain fewer than `min_samples_split` samples.
- **Purpose**: Prevents the tree from growing too deep, which could lead to overfitting. By limiting the depth, you can control the model’s complexity and potentially improve generalization.

### 2. **min_samples_split**

- **Definition**: Specifies the minimum number of samples required to split an internal node. If a node has fewer than `min_samples_split` samples, it will not be split further.
- **Purpose**: Controls the minimum size of the nodes that are allowed to split. A larger value will result in a simpler tree because it restricts the splitting, leading to fewer nodes. A smaller value might lead to overfitting, as the tree might grow too complex.

### 3. **min_samples_leaf**

- **Definition**: Sets the minimum number of samples required to be at a leaf node. If a leaf node has fewer than `min_samples_leaf` samples, it will not be split.
- **Purpose**: This parameter helps in smoothing the model, as it prevents the model from creating leaf nodes with very few samples, which could lead to overfitting. It forces each leaf node to have a sufficient number of samples, promoting generalization.

### 4. **min_weight_fraction_leaf**

- **Definition**: Specifies the minimum weighted fraction of the total sum of weights required to be in a leaf node. This is a weighted version of `min_samples_leaf` used when sample weights are provided.
- **Purpose**: It ensures that the leaf nodes contain enough weighted samples. This is useful in cases where the sample weights differ significantly, and you want to prevent leaves from containing too few weighted samples.

### 5. **max_leaf_nodes**

- **Definition**: Limits the number of leaf nodes in the tree. If this parameter is set to an integer value, the tree will be pruned (if necessary) to ensure the number of leaf nodes does not exceed the specified value.
- **Purpose**: Controls the complexity of the tree by limiting the number of leaf nodes. Pruning the tree to have a fixed number of leaf nodes can help avoid overfitting.

### 6. **min_impurity_decrease**

- **Definition**: Specifies the minimum impurity decrease required for a split to occur. A split will only happen if the impurity decrease is greater than or equal to this threshold.
- **Purpose**: This helps to control how "clean" the splits must be. A higher value leads to fewer splits, resulting in a simpler tree. This can prevent the tree from overfitting by making the splits more meaningful and significant.

### 7. **min_impurity_split** (Deprecated)

- **Definition**: This parameter was used to specify the threshold for the impurity of a node to allow further splitting. It was an older version of the `min_impurity_decrease` parameter and has been deprecated.
- **Purpose**: It was originally used to limit further splitting when a node's impurity is lower than a certain threshold. It's now recommended to use `min_impurity_decrease` instead.

### Summary of the Parameters' Roles:

- **max_depth** and **max_leaf_nodes**: Control the overall size and complexity of the tree.
- **min_samples_split** and **min_samples_leaf**: Control the minimum number of samples required to split a node or form a leaf, helping to avoid overfitting.
- **min_weight_fraction_leaf**: Used when weighted samples are involved, controlling the minimum weight fraction required for a leaf node.
- **min_impurity_decrease**: Controls the required impurity decrease for a split to occur, influencing the tree’s complexity.
- **min_impurity_split** (deprecated): Historically controlled splitting based on impurity thresholds but is now replaced by `min_impurity_decrease`.

By adjusting these parameters, you can fine-tune your decision tree to balance underfitting and overfitting, helping it generalize better to unseen data.


---

# Decision Tree Classifier for Breast Cancer Prediction

## Importing Dependencies
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Importing data
from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()

# Creating a DataFrame
df = pd.DataFrame(data=data['data'], columns=data['feature_names'])

# Display the first 5 rows of the dataframe
df.head()
```

## Exploratory Data Analysis (EDA)
```python
df.info()
df.describe()
```

## Splitting the Data
```python
from sklearn.model_selection import train_test_split

x = df
y = data['target']

print(x.shape, y.shape)

# Splitting into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)

print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)
```

## Model Selection and Training
```python
# Import Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

# Creating an instance of the classifier
dtc = DecisionTreeClassifier()

# Training the model
dtc.fit(x_train, y_train)

# Display model parameters
dtc.get_params()

# Making predictions
y_pred = dtc.predict(x_test)
y_pred
```

## Getting Probability Predictions
```python
dtc.predict_proba(x_test)
```

### Observation
- The probabilities are all 1 or 0.
- This indicates overfitting as the tree has too much freedom.

## Reducing Overfitting with Depth Control
```python
# Limiting the degree of freedom
model = DecisionTreeClassifier(max_depth=4)
model.fit(x_train, y_train)
y_pred_2 = model.predict(x_test)

# Probability predictions with depth control
model.predict_proba(x_test)
```

## Model Evaluation
### Classification Report
```python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_2, target_names=["malignant", "benign"]))
```

### Confusion Matrix
```python
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred_2)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()
```

### Displaying the Confusion Matrix
```python
from sklearn.metrics import ConfusionMatrixDisplay

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)
disp.plot(cmap="BuPu")
plt.title("Confusion Matrix")
plt.show()
```

### Evaluation Metrics
```python
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

# Calculating errors
error_dict = {
    "accuracy": accuracy_score(y_test, y_pred_2),
    "recall": recall_score(y_test, y_pred_2, average="macro"),
    "precision": precision_score(y_test, y_pred_2, average="macro"),
    "f1_score": f1_score(y_test, y_pred_2, average="macro")
}

for metric, value in error_dict.items():
    print(f"{metric}: {value}")
```

## Decision Tree Parameters Controlling Splitting
```plaintext
- max_depth
- min_sample_split
- min_sample_leaf
- min_weight_fraction_leaf
- max_leaf_nodes
- min_impurity_decrease
- min_impurity_split
```

## Decision Tree Criteria and Splitting
```plaintext
- Criteria: GINI, Entropy
- Splitter: Best, Random
- max_features: Number of features to split on
- random_state
```

## Class Weight
```plaintext
- Class weight is used to balance the importance of classes during training.
- Useful in cases of class imbalance.
```

## Feature Importance
```python
# Feature importance analysis
features = x.columns
features_importance = pd.DataFrame(data=dtc.feature_importances_, index=features, columns=["value"]).sort_values(by='value', ascending=False)

# Displaying feature importance
features_importance

# Plotting top 10 features
features_importance.head(10).plot(kind='barh')
plt.show()
```

## Decision Tree Visualization
```python
from sklearn.tree import plot_tree

# Visualizing the decision tree
fig = plt.figure(figsize=(25, 20))
plot_tree(model, feature_names=list(x.columns), class_names=['malignant', 'benign'], filled=True, fontsize=12)
plt.show()
```

## Pruning the Decision Tree

```python
# Pruning using ccp_alpha
model = DecisionTreeClassifier(ccp_alpha=0.01)
model.fit(x_train, y_train)
y_pred_3 = model.predict(x_test)

# Classification report after pruning
print(classification_report(y_test, y_pred_3, target_names=["malignant", "benign"]))

# Visualizing the pruned tree
fig = plt.figure(figsize=(25, 20))
plot_tree(model, feature_names=list(x.columns), class_names=['malignant', 'benign'], filled=True, fontsize=12)
plt.show()
```

---

# resources

- https://www.kdnuggets.com/essential-data-cleaning-techniques-accurate-machine-learning-models
- https://sebastianraschka.com/faq/docs/decision-tree-binary.html
- https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html
- https://www.youtube.com/watch?v=_L39rN6gz7Y (statquest)