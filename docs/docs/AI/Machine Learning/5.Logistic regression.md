---
author: Mostafa Samir Ali
tags:
  - AI
  - Machine_learning
---

# Logistic regression

- logistic regression is binary classification

## why we cannot apply/use linear regression in classification?

* because the line equation is sensitive to outliers (if an outlier happen to be in the data --> values will be affected)
* line equation(Linear regression) is used to predict continuous values , while classification is discrete values
* in regression if the error between values is too low then its a good model , while in classification its too sensitive to errors between values
	- example: if the actual value = 3.2 and the predicted = 3.1 --> in regression its too good only 0.1 error
	- in classification if we have 2 values [0] or [1] and the predicted is 0.1 --> completely wrong (value do not exist)

## What is Logistic Regression?

- Logistic regression is used for binary classification where we use **sigmoid function**, that takes input as independent variables and produces a probability value between 0 and 1.

- For example, we have two classes Class 0 and Class 1 if the value of the logistic function for an input is greater than 0.5 (threshold value) then it belongs to Class 1 otherwise it belongs to Class 0. 
- It’s referred to as regression because it is the extension of Linear regression but is mainly used for classification problems.

- logistic regression have (S) shape --> that solved the outlier problem in the linear regression
- if an outlier appeared in the data you need to extend the the upper wischer of the S shape

- in mathimatics to drae this S shape it has a function called "Sigmoid" function
$$sigmoid = \frac{1}{1\ +\ e^{-z}}$$
- where $z$ = $m.x+b$ Linear regression (line equation)
<br><br>
- calculation of error in linear regression is different than logistic regression
- if we use squared error in logistic regression --> it will lead to non-convex shape of error function that will lead to missing the global minimum and may be trapped with local minimum

## Key Points:

- Logistic regression predicts the output of a <u>categorical dependent variable.</u> 
- Therefore, the outcome must be a categorical or discrete value.
- It can be either Yes or No, 0 or 1, true or False , but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.
- In Logistic regression, instead of fitting a regression line, **we fit an “S” shaped logistic function**, which predicts two maximum values (0 or 1)
- hypothesis means the equations 
- in linear regression the fit line is called line **_WHILE_** in logistic regression it's called <u>"decesion boundry"</u> 
- the range of x-axis is $(-\infty\ ,+\infty)$ while the range in y-axis is (0 , 1) 
- each time the $z$ value increase the S shape become more shape

## Assumptions of Logistic Regression

We will explore the assumptions of logistic regression as understanding these assumptions is important to ensure that we are using appropriate application of the model. The assumption include:

1. Independent observations: Each observation is independent of the other. meaning there is no correlation between any input variables.
2. Binary dependent variables: It takes the assumption that the dependent variable must be binary or dichotomous, meaning it can take only two values. For more than two categories SoftMax functions are used.
3. Linearity relationship between independent variables and log odds: The relationship between the independent variables and the log odds of the dependent variable should be linear.
4. **No outliers: There should be no outliers in the dataset.**
5. Large sample size: The sample size is sufficiently large

## Terminologies involved in Logistic Regression

Here are some common terms involved in logistic regression:

- ****Independent variables:**** The input characteristics or predictor factors applied to the dependent variable’s predictions.
- ****Dependent variable:**** The target variable in a logistic regression model, which we are trying to predict.
- ****Logistic function:**** The formula used to represent how the independent and dependent variables relate to one another. The logistic function transforms the input variables into a probability value between 0 and 1, which represents the likelihood of the dependent variable being 1 or 0.
- ****Odds:**** It is the ratio of something occurring to something not occurring. it is different from probability as the probability is the ratio of something occurring to everything that could possibly occur.
- ****Log-odds:**** The log-odds, also known as the logit function, is the natural logarithm of the odds. In logistic regression, the log odds of the dependent variable are modeled as a linear combination of the independent variables and the intercept.
- ****Coefficient:**** The logistic regression model’s estimated parameters, show how the independent and dependent variables relate to one another.
- ****Intercept:**** A constant term in the logistic regression model, which represents the log odds when all independent variables are equal to zero.
- ***Maximum likelihood*** The method used to estimate the coefficients of the logistic regression model, which maximizes the likelihood of observing the data given the model.

# calculating error for logistic regression
- if y = 1 , $$-log(h_\theta(x)$$
- if y = 0 , $$-log(1-h_\theta(x))$$
- we can merge the 2 equations in one equation
$$Cost(h_\theta(x),y) = -y.log(h_\theta(x))\ -\ (1-y).log(1-h_\theta(x))$$
- we merged the 2 equations to reach the **convex shape** of the cost function

![[images/Untitled 1.png| center]]

- equation of gradient descent for this cost function
$$\frac{1}{m}\ \sum_{i=1}^m\ y.log(h_\theta(x))\ +\ (1-y).log(1-h_\theta(x))$$
- to decrease the value of the cost function , we take the derivative $\partial$ and assign it to 0
- the new fom for temp variable in optimizer algorithm
$$\left(\sum\ h_\theta(x^{(i)})\ -\ y^i\right).x^i_j$$
$$temp_j = \theta_j - \alpha . \frac{1}{m}\left(\sum\ h_\theta(x^{(i)})\ -\ y^i\right).x^i_j$$
# Logistic regression approaches
- logistic regression is binary and may work as multi classification
- 2 ways for logistic regression to work
    - ove vs rest
    - one vs one

### one vs rest logistic regression
---
 - if we have 3 classes , the logistic regression will try to separate all of them by O.vs.R  methodology
 - it will consider the first class as one class and the rest of classes as one class --> (classifier 1) 
 - then calculate the likelyhood of this point to belong to this class (%)
 - it will go to the second class consider it as a separate class and the others are one class , the same for the third class --> (classifier 2 ) (classifier 3 ) , calculate the likelihood for both of them
 - after finishing , it will consider the point belong to  a specific class according to the highest likelihood
 - **number of classifiers = no. classes**

![[Untitled.jpg | center]]

### one vs one (multi class classification)
---
- it works the same way as 
- instead of taking all classes in consider , it only takes 2 classes in each classifier
    - it takes 2 classes 
    - make a classifier
    - calculate the probability
- repeat the previous steps for $\frac{n.(n-1)}{2}$
- **no of classifiers = $\frac{n(n-1)}{2}$**

![[Untitled 1.jpg | center]]

# CODE
#### import dependencies 
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
```

#### Load data
```python
iris = load_iris()
x = pd.DataFrame(iris["data"] , columns = iris["feature_names"])
y = pd.DataFrame(iris["target"]).values

# split the data
x_train1 , x_test1 , y_train1 , y_test1 = train_test_split(x , y , test_size=.2 , random_state=42)
```

#### scaling the data
```python
from sklearn.preprocessing import StandardScaler
scale = StandardScaler()

x_train2 = scale.fit_transform(x_train1)
x_test2 = scale.transform(x_test1)
```

#### model
```python
model = LogisticRegression()
model.fit(x_train2 , y_train1)

# making predictions
y_pred1 = model.predict(x_test2)
```

#### measure error

```python
from sklearn.metrics import accuracy_score , recall_score , precision_score , f1_score

acc = accuracy_score(y_pred1 , y_test1)
recall = recall_score(y_pred1 , y_test1 , average="macro") # why avr = macro ???
pre =precision_score(y_pred1 , y_test1 , average="macro")
f1 = f1_score(y_pred1 , y_test1 , average="macro")

my_errors = [acc , recall , pre , f1]

for i in range(len(my_errors)):
    print(f"error {i+1} is {my_errors[i]}")
```

![[Screenshot 2024-10-28 213851.png]]

