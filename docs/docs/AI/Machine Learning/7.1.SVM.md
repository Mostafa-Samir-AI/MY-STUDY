---
author: Mostafa Samir Ali
tags:
  - Machine_learning
---

# intro to margin classifiers and SVC
- lets start a basic Classification problem
- we want to divide two classes away of each other ex (person obese or not obese)
- so we take measurements and plot the data , then make a threshold to divide the data

![[Screenshot 2025-01-22 085858.png | center]]

- with this threshold we can classify data point into 2 classes (obese and not obese)
- if we got a new data point we easily classify it , look in which side it occurs

![[Screenshot 2025-01-22 090330.png | center]]

- at the above picture we can see that the new data point is classified into **obese** because it's on the right side of the threshold
- but , it doesn't seem quite right , because <u>the new data point is a lot near to the **not obese** class</u> 
- so this threshold is does not represent the data correctly
- we need another representing threshold
<br><br>
- now when we return to our observations we can take outer most observations positions , take measurements and then <u>take the midpoint between them as new threshold</u> 

![[Screenshot 2025-01-22 090808.png | center]]

![[Screenshot 2025-01-22 090818.png | center]]

- this method of classification is called **maximum margin classifier** 

> [!warning]+ 
> - maximum margin classifiers are super sensitive to outliers than can lead to high variance 
> 
> ![[Screenshot 2025-01-22 091926 1.png | center]]
> 
> ![[Screenshot 2025-01-22 091938.png | center]]

- can we do better , YES ! 
- we can set the another values for the margin and allow misclassification 
- this approach called **SOFT MARGIN** 

![[Screenshot 2025-01-22 092424.png | center]]

>[!faq]+
> - there are a lot of soft margins we can try 
> - which one is the best 
> - we use cross validation to determine the best one 

- when we use soft margin as a classifier this is called **soft margin classifier** or **support vector classifier**
- **support vector classifier** : the observations on the edge and within the soft margin called **support vectors**

![[Screenshot 2025-01-22 092957.png | center]]

>[!NOTE]
> - if the data is 1-D then the SVC is single point on a 1-D number line (0 dimensional subspace)
> - if the data is 2-D then the SVC is 1-D line on 2-D number line (1 dimensional subspace)
> - if the data is 3-D then the SVC is 2-D plane on a 3-Dnumberline (2 dimensional subspace)

---
- now the above data is easily separable 
- what will happen if we have a data that have tons of overlapping 

![[Pasted image 20250125142015.png | center]]

- no matter we put the classifier we will make a lot of classification
- since SVC and margin classifiers are bad at dividing this data we need to use SVM

# Support Vector Machines
- the main idea behind support vector machines are
	1. start with data in a very low dimension 
	2. move the data in a higher dimension 
	3. find the SVC that classifies separates the data

### 1. start with data in a very low dimension 

![[Pasted image 20250125143103.png | center]]

### 2. move the data in a higher dimension

![[Screenshot 2025-01-25 143132.png | center]]


### 3. find the SVC that classifies separates the data

![[Screenshot 2025-01-25 143156.png | center]]

## How to transform the data
- which dimension should be picked to transform the data 
- in this case SVM uses **kernel functions**
	- in the above example we used **Polynomial kernel** which have parameter `d` which stands for degree
	- when `d = 1` the polynomial kernel computes the relationship between each pair of observation in 1-D
	- and the relationships is used to find the SVC

![[Pasted image 20250125144053.png | center]]


>[!faq]+
> - to reduce the calculations and computational power
> - kernel function compute the relationship between points as it is in a higher dimension 
> - they don't actually do the transformation
> - SVM uses kernel Tricks
> 
> ![[Pasted image 20250125144511.png | center]]





>[!warning]
>- binning















---
# resources
- https://medium.com/low-code-for-advanced-data-science/support-vector-machines-svm-an-intuitive-explanation-b084d6238106
- https://www.youtube.com/watch?v=efR1C6CvhmE&t=456s
- https://developers.google.com/machine-learning/crash-course/numerical-data/binning (binning)