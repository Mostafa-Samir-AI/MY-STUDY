# Big O 
## what is big O ?
**Big O** : a metric used to describe efficiency of a code 
- if you do not know where your algorithm gets faster , then you will struggle to judge your code performance 

![[Screenshot 2024-10-26 220749.png | center]]
### Time complexity

**Time complexity** : It's a way of showing how the runtime of the function increases as the size of the input increases.
- its used to measure the performance of code according to the runtime. 
- *but* when we measure time complexity <u>we do not use time as metric</u> instead we consider the **number of operation as the true metrics**  
- So time complexity depend on *number of operations* in our code 

### Space complexity 
**Space complexity** : the amount of memory that the code has used 
- when dealing with space limited systems , space complexity comes as useful metric to design the algorithm

![[Screenshot 2024-10-26 221134.png]]
###### Both time and space complexity are important in algorithm design 

---
# Big O Notations
- any algorithm might behave differently in each condition
- we will discuss three scenarios in case of measuring any algorithm 

## Three  Scenarios 
### 1- Big Omega ($\omega$)
- **best case** that an algorithm can operate
- **Big Omega** is going to be the complexity which is going to be the least more than the best case 
### 2- Big Theta ($\theta$)
- **average case** that an algorithm can operate 
- **big theta** is going to be the complexity within the bounds of first and best case.
### 3- Big Omicron ($O$)
- **worst case** an algorithm can operate
- **big o** is going to be the complexity that's going to be the less or equal to the worst case

#### `big o and big theta are used in academics but in industry we use big o`

---

# commonly used complexities in *Runtime*

## 1. Constant Time Complexity O(1)
- at any given input the execution will not change , it will remain constant
- it means <u>number of operations will be the same (fixed)</u>
- for example
```python
# function to calculate multiplication
def multi(x1 , x2):
	return x1 * x2

out = multi(10,20)
print(out)
```
- the output will be `200` 
- what happened here is that we designed a function that calculate the multiplication of 2 numbers 
- note that the *number of operations will be the same* , So we call it **constant runtime complexity**   
- No matter how many input or the size of the input *the number of operations will be the same* 
- this is the most efficient time complexity
## 2. Linear Time Complexity O(n)
- <u>the time complexity will grow in proportion with the size of the input </u>
- the best practice is to keep our function running below or within this range 
- BUT this is not always possible
- for example
```python
def print_item(n):
	for i in range(n):
		print(i)

print_item(10)
```
- the output will be `1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9`
- the *no. operation* will increase *n* times 

> [!Tip] 
> ## Drop Constant 
> - <u>dropping constant in asymptotic analysis</u>
> - <u>because $Big O$ measures the rate of increase not the amount</u>
> - this make the algorithm analysis more effective and remove complexity in analysis (remove complexity from the analysis)
> - As n approaches to infinity, constant factors are not really a big deal.
> - *for example*
> ```python
> def print_item(n):
> 	for i in range(n):
> 		print(i)
> 	for j in range(n):
> 		print(j)
> 
> # both the loops have o(n)
> # so its O(2n) --> removing the constant ==> O(n)
> 
> # testing the function
> print_item(10)
> 
> ```
> 

> [!NOTE] # Note
> - constants are stored in the memory
> - memory is accessed by different rates in each computer , and that's a hardware business not an algorithm issue
> - so when we drop constants we neglect the hardware issues focusing on the algorithm analysis

## 3. O(n<sup>2</sup>) Quadratic complexcity
- <u>represents the complexity of an algorithm, whose performance is proportional to the square of the size of the input elements.</u>
- its like running a loop inside a loop 
- *for example*
```python
def print_item(n):
	for i in range(n):
		for j in range(n):
			print(i,j)

# for each loop we will have O(n) operations
# but for each iteration in the first loop we will have o(n) from second loop 
# simplified n*n = n.power(2)

# testing 
print_items(10) # it will iterate 100 times 
```
- output `(0,0) ==> (9,9)`

## 4. Non-Dominant Term
- *dropping* the non-dominant term in case of *Big(O)* , because it does not affect the complexity alot
- *for example*
```python
def print_item(n):
	for i in range(n): # complexity o(n2)
		for j in range(n):
			print(i,j)
	for k in range(n): # complexity o(n)
		print(k)

# testing
print_item(10)
```
- the total complexity will be **O(n<sup>2</sup>+n)** 
- after dropping it will be **O(n<sup>2</sup>)**  ONLY !
###### if we look at it 
- the algorithm will operate in the first loop 10,000 times (if n = 100)
- and the second loop (loop of K) will operate 100 times (*neglected*)

## 5. O(log n)
- as the input size grows, the number of operations grows very slowly
- its like divide and conquer algorithm
#### Explaining
- like if we have an array `[1,2,3,4,5,6,7,8]` and we need to find any number
- for instance we need to find the `1` 
- so we will divide the array into *2 part* `1,2,3,4` and `5,6,7,8` 
- then , we will choose one of them to search in it / we gonna choose the first part
- we will divide again into *2 parts*  `1,2` and `3,4`
- again dividing `1` and `2` , **Finally** we found our target after dividing *3 times*
- so the time complexity for this algorithm will be **O(log<sub>2</sub> 8)** = 3 OR **2<sup>3</sup>**  = 8 
$$\log_28 =\ ?\ \ \ \log_28 =\ 3$$



 _this technique will be useful when dealing with array with huge number of elements like an array of million element_
 - an array with million element will take 20 steps to find a specific number 
 - 2<sup>20</sup> = 1,048,576 element 
---
# Time complexity Rules
![[Pasted image 20250209114135.png | center]]

---
# Space Complexity
- <u>the measure of amount of storage the working algorithm need</u>
- <u>how much memory needed in the worst case at any point in the algorithm</u>
- we are also concerned with how will the space will grow as a response of input growth
<br><br>
- ex
```python
def sum(n):
	if n <= 0:
		return 0
	return n+sum(n-1)
```

- inserting function in the memory stack

![[Screenshot 2025-02-09 104241.png | center]]

---

